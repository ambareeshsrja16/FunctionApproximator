{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_squares_csv(lower_limit = 1, upper_limit=1000, filename = \"Squares_1_1000.csv\"):\n",
    "    \"\"\"Create a csv file with numbers and their squares\"\"\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        for num in range(lower_limit, upper_limit+1):\n",
    "            print(str(num) + \" \" + str(num**2), file=f)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_squares_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaresDataset(Dataset):\n",
    "    def __init__(self, filename=\"Squares_1_1000.csv\"):\n",
    "        super().__init__()              \n",
    "        with open(filename, 'r') as f:\n",
    "            self.samples = f.read().splitlines()\n",
    "        \n",
    "        for index, point in enumerate(self.samples):\n",
    "            self.samples[index] = [int(num) for num in point.split()]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionApproximator(nn.Module):\n",
    "    def __init__(self, layers=2, hidden_units= [6,4], non_linearity = \"relu\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_list = nn.ModuleList()\n",
    "        self.layer_list.append(nn.Linear(1, hidden_units[0]))\n",
    "        \n",
    "        for layer in range(layers-1):\n",
    "            self.layer_list.append(nn.Linear(hidden_units[layer], hidden_units[layer+1]))\n",
    "            \n",
    "        self.layer_list.append(nn.Linear(hidden_units[-1],1))\n",
    "        \n",
    "        if non_linearity == 'relu':\n",
    "            self.non_linearity = torch.relu\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        \n",
    "        for index, layer in enumerate(self.layer_list[:-1]):\n",
    "            h = layer(h)\n",
    "            h = self.non_linearity(h)\n",
    "        \n",
    "        output =  self.layer_list[-1](h)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import torch.utils.data as td\n",
    "    from torch.optim import Adam\n",
    "    \n",
    "    RANDOM_SEED = 18\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        torch.cuda.manual_seed(RANDOM_SEED)\n",
    "        torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "        kwargs = {'num_workers': 0, 'pin_memory': True}\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        torch.manual_seed(RANDOM_SEED)\n",
    "        torch.set_default_tensor_type(torch.FloatTensor)\n",
    "        kwards = {}\n",
    "        \n",
    "    trainset = SquaresDataset(filename=\"Squares_1_1000.csv\")\n",
    "    testset = SquaresDataset(filename=\"Squares_1_1000.csv\") #TODO Change to different numbers\n",
    "\n",
    "    LR = 3e-4 # It's Magic! https://twitter.com/karpathy/status/801621764144971776?lang=en\n",
    "    DATAPOINTS = len(trainset)\n",
    "    BATCH_SIZE = 512\n",
    "    BATCHES = DATAPOINTS / BATCH_SIZE\n",
    "\n",
    "    net = FunctionApproximator(layers=2, hidden_units= [6,4], non_linearity = \"relu\")\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    net = net.to(device)\n",
    "\n",
    "    train_loader = td.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "    val_loader = td.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "\n",
    "    optimizer = Adam(net.parameters(), lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 25]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 10201]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-96635fe7d3cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "def train_model(start_steps = 0, end_steps = 5, net=None, model_name = \"prelim\", \n",
    "                train_loader = train_loader, val_loader = val_loader, \n",
    "                logger_level = 20):\n",
    "    \n",
    "    for epoch in tqdm(range(start_steps, end_steps)):\n",
    "        count = epoch-start_steps+1\n",
    "        net.train()\n",
    "        \n",
    "        #Epoch begins\n",
    "        epoch_loss = 0.0\n",
    "        for d in tqdm(train_loader):\n",
    "            x, y = d.split()\n",
    "            x, y = int(x), int(y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            with torch.no_grad():\n",
    "                d = d.to(device)\n",
    "            y = net(x)\n",
    "            loss = torch.sqrt(criterion(y, d))   #RMSE Loss   \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            #Break if NaN encountered\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                logging.info(f\"Loss value: {loss.item()}\")\n",
    "                logging.info(\"NaN/inf occured at:\")\n",
    "                logging.info(f\"{x}\\n\")\n",
    "                logging.info(f\"{d}\\n\")\n",
    "                logging.info(f\"Original x was : {original_x}\")\n",
    "                NaN_flag = True\n",
    "                break\n",
    "\n",
    "            logging.debug(f\"Count: {count}, Loss :{loss}\")\n",
    "            \n",
    "        if NaN_flag: break   #Stop training if NaN encountered\n",
    "        \n",
    "        #Print to screen every few epochs    \n",
    "        if count%LOG_INTERVAL == 0:\n",
    "            print(f\"Epoch number:{epoch} Loss: {epoch_loss:.4f}\")  \n",
    "            \n",
    "        #Training artifacts\n",
    "        if model_name not in os.listdir():\n",
    "            os.makedirs(model_name+\"/artifacts/saved_model/\")\n",
    "\n",
    "        #Write to loss file every epoch\n",
    "        with open(model_name+\"/artifacts/loss_curve\",mode = 'a+') as f:\n",
    "            f.write(f\"Epoch_number: {epoch} Loss: {epoch_loss:.4f}\\n\")\n",
    "            \n",
    "        #Validation curve\n",
    "        val_loss = 0.0\n",
    "        net.eval()\n",
    "        for x,d in val_loader:\n",
    "            x[torch.isnan(x)]=0\n",
    "            d[torch.isnan(d)]=0\n",
    "            x = x.to(device)\n",
    "            with torch.no_grad():\n",
    "                d= d.to(device)\n",
    "            y = net(x)\n",
    "            loss = torch.sqrt(criterion(y,d))\n",
    "            val_loss+=loss\n",
    "        net.train()\n",
    "        #Write Val loss to file every epoch\n",
    "        with open(model_name+\"/artifacts/val_loss_curve\",mode = 'a+') as f:\n",
    "            f.write(f\"Epoch_number: {epoch} Loss: {val_loss:.4f}\\n\")\n",
    "        \n",
    "        #Save model every few epochs\n",
    "        if epoch%SAVE_INTERVAL== 0:\n",
    "            torch.save(net.state_dict(),f\"./{model_name}/artifacts/saved_model/model_at_epoch{epoch}\")\n",
    "        #Epoch Ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
